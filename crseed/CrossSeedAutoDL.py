from asyncio.streams import FlowControlMixin
import json
import logging
import os
import re
from django.utils import timezone
import requests
import shutil
import time
from guessit import guessit
from urllib.parse import urlencode
from .models import CrossTorrent, TaskControl, SearchedHistory

logger = logging.getLogger(__name__)

# logger.setLevel(logging.DEBUG)
# formatter = logging.Formatter('\n%(asctime)s - Module: %(module)s - Line: %(lineno)d - Message: %(message)s')
# file_handler = logging.FileHandler('CrossSeedAutoDL.log', encoding='utf8')
# file_handler.setFormatter(formatter)

# logger.addHandler(file_handler)


class Searcher:
    # 1 MibiByte == 1024^2 bytes
    MiB = 1024**2
    # max size difference (in bytes) in order to account for extra or missing files, eg. nfo files
    size_differences_strictness = {True: 0, False: 5 * MiB}
    # max_size_difference = size_differences_strictness[ARGS.strict_size]
    max_size_difference = size_differences_strictness[False]

    # keep these params in response json, discard the rest
    keys_from_result = [
        'Tracker', 'TrackerId', 'CategoryDesc', 'Title', 'Link', 'Details',
        'Category', 'Size', 'Imdb'
    ]
    # torznab categories: 2000 for movies, 5000 for TV. This dict is for matching against the (str) types generated by 'guessit'
    category_types = {'movie': 2000, 'episode': 5000}

    def __init__(self, process_param):
        self.search_results = []
        self.process_param = process_param

    def search(self, local_release_data, log):
        if local_release_data['size'] is None:
            s = 'Skipped: Could not get proper filesize data'
            logger.info(s)
            log.message(s)
            return []

        search_query = local_release_data['guessed_data']['title']
        if local_release_data['guessed_data'].get('year') is not None:
            search_query += ' ' + str(
                local_release_data['guessed_data']['year'])

        search_url = self._get_full_search_url(search_query,
                                               local_release_data)
        logger.info(search_url)

        resp = None
        for n in range(2):
            try:
                resp = requests.get(search_url, local_release_data)
                break
            except requests.exceptions.ReadTimeout:
                if n == 0:
                    s = 'ERROR: Connection timed out. Retrying once more.'
                    print(s)
                    log.message(s, error_abort=1)
                    time.sleep(self.process_param.delay)
            except requests.exceptions.ConnectionError:
                if n == 0:
                    s = 'ERROR: Connection failed. Retrying once more.'
                    print(s)
                    log.message(s, error_abort=1)
                    time.sleep(self.process_param.delay)

        if not resp:
            s = 'ERROR: No response, check the Jackett setting.'
            log.message(s, error_abort=1)
            return []
        ###
        # self._save_results(local_release_data); exit()
        try:
            resp_json = resp.json()
        except json.decoder.JSONDecodeError as e:
            print('Json decode error. Incident logged')
            s = 'ERROR: Json decode Error. Response text: ' + resp.text
            logger.info(s)
            logger.exception(e)
            log.message(s)
            return []

        if resp_json['Indexers'] == []:
            info = 'ERROR: No results found due to incorrectly input indexer names ({}). Check ' \
                   'your spelling/capitalization. Are they added to Jackett? Exiting...'.format(self.process_param.trackers)
            print(info)
            logger.info(info)
            log.message(info, error_abort=1)
            return []

        # # append basename to history
        # if local_release_data['basename'] not in search_history['basenames_searched']:
        #     search_history['basenames_searched'].append(local_release_data['basename'])

        self.search_results = self._trim_results(resp_json['Results'])
        return self._get_matching_results(local_release_data, log)

    # construct final search url
    def _get_full_search_url(self, search_query, local_release_data):
        base_url = self.process_param.jackett_url.strip(
            '/') + '/api/v2.0/indexers/all/results?'

        main_params = {
            'apikey': self.process_param.jackett_api_key,
            'Query': search_query
        }

        optional_params = {
            'Category[]':
            Searcher.category_types[local_release_data['guessed_data']
                                    ['type']],
            'season':
            local_release_data['guessed_data'].get('season'),
            'episode':
            local_release_data['guessed_data'].get('episode')
        }
        if self.process_param.trackers.strip():
            optional_params['Tracker[]'] = self.process_param.trackers

        for param, arg in optional_params.items():
            if arg is not None:
                main_params[param] = arg

        return base_url + urlencode(main_params)

    def _get_matching_results(self, local_release_data, log):
        matching_results = []
        # print(f'Parsing { len(self.search_results) } results. ', end='')

        for result in self.search_results:
            max_size_difference = self.max_size_difference
            # older torrents' sizes in blutopia are are slightly off
            if result['Tracker'] == 'Blutopia':
                max_size_difference *= 2

            if abs(result['Size'] -
                   local_release_data['size']) <= max_size_difference:
                matching_results.append(result)

        s = f'{ len(matching_results) } matched of { len(self.search_results) } results.'

        logger.info(s)
        log.message(s)

        return matching_results

    # remove unnecessary values from results json
    def _trim_results(self, search_results):
        trimmed_results = []

        for result in search_results:
            new_result = {}
            for key in self.keys_from_result:
                new_result[key] = result[key]
            new_result['Title'] = self._reformat_release_name(
                new_result['Title'])
            trimmed_results.append(new_result)
        return trimmed_results

    # some titles in jackett search results get extra data appended in square brackets,
    # ie. 'Movie.Name.720p.x264 [Golden Popcorn / 720p / x264]'
    @staticmethod
    def _reformat_release_name(release_name):
        release_name_re = r'^(.+?)( \[.*/.*\])?$'

        match = re.search(release_name_re, release_name, re.IGNORECASE)
        if match:
            return match.group(1)

        logger.info(f'"{release_name}" name could not be trimmed down')
        return release_name

    ###
    # def _save_results(self, local_release_data):
    #     search_results_path = os.path.join( os.path.dirname(os.path.abspath(__file__)), 'search_results.json' )
    #     target_dict = {'local_release_data': local_release_data, 'results': self.search_results}
    #
    #     with open(search_results_path, 'w', encoding='utf8') as f:
    #         json.dump([target_dict], f, indent=4)


def genSearchKeyword(basename, size, log):
    local_release_data = {
        'basename': basename,
        'size': size,
        'guessed_data': guessit(basename)
    }

    if local_release_data['guessed_data'].get('title') is None:
        s = 'Skipped: Could not get title from filename: {}'.format(
            local_release_data['basename'])

        logger.info(s)
        log.message(s)

        return []

    if re.search(r'[\u4e00-\u9fa5\u3041-\u30fc]',
                 local_release_data['guessed_data']['title']):
        s = 'Skipped: contains CJK characters in title: {}'.format(
            local_release_data['basename'])
        logger.info(s)
        log.message(s)

        return []

    return local_release_data


def isPreviouslySearched(torName):
    return SearchedHistory.objects.filter(name=torName).exists()


def saveSearchedTorrent(st):
    newSt = SearchedHistory()
    newSt.hash = st.torrent_hash
    newSt.name = st.name
    newSt.size = st.size
    newSt.location = st.save_path
    newSt.tracker = st.tracker
    newSt.added_date = st.added_date
    newSt.save()
    return newSt
    # newSt.status = status


def saveCrossedTorrent(st, searchTor):
    newCt = CrossTorrent()
    newCt.hash = st.torrent_hash
    newCt.name = st.name
    newCt.size = st.size
    newCt.location = st.save_path
    newCt.crossed_with = searchTor
    newCt.tracker = st.tracker
    newCt.added_date = st.added_date
    newCt.save()
    return newCt


def downloadResult(dlclient, result, localTor, log):
    if result['Link'] is None:
        s = 'Skipped: - Skipping release (no download link): ' + localTor.name
        logger.info(s)
        log.message(s)
        return None
    s = 'Grabbing release: ' + localTor.name
    logger.info(s)
    log.message(s)
    return dlclient.addTorrentUrl(result['Link'], localTor.save_path)


def checkTaskCanclled():
    taskctrl = TaskControl.objects.all().last()
    if not taskctrl:
        return False
    else:
        return taskctrl.cancel_task


def iterTorrents(dlclient, process_param, log):
    FlowControlLimitCount = process_param.fc_count  # count limit per button press
    FlowControlInterval = process_param.fc_interval  # seconds between query
    log.status(flow_limit=FlowControlLimitCount)

    log.message('Loading torrents in the client')
    torList = dlclient.loadTorrents()
    log.status(total_in_client=len(torList))

    for_count = 0
    query_count = 0
    for localTor in torList:
        
        for_count += 1
        log.status(progress=for_count)
        if checkTaskCanclled() or log.abort():
            return 

        if isPreviouslySearched(localTor.name):
            continue

        dbSearchTor = saveSearchedTorrent(localTor)
        searchData = genSearchKeyword(localTor.name, localTor.size, log)
        if not searchData:
            continue
        
        query_count += 1
        log.status(progress=for_count, query_count=query_count)
        log.message('Searching: ' + searchData['guessed_data']['title'])
        if query_count >= FlowControlLimitCount:
            return
        searcher = Searcher(process_param)
        matchingResults = searcher.search(searchData, log)
        log.inc(match_count=len(matchingResults))
        # log.message('Found: %d Results' % (len(matchingResults)))
        for result in matchingResults:
            if checkTaskCanclled() or log.abort():
                return 
            st = downloadResult(dlclient, result, localTor, log)
            if st:
                print(f'- Success added: {localTor.name}')
                logger.info(f'- Success added: {localTor.name}')
                log.inc(download_count=1)
                log.message('Added: ' + localTor.name)
                saveCrossedTorrent(st, dbSearchTor)
            else:
                log.message('Skiped: ' + localTor.name)

        time.sleep(FlowControlInterval)
